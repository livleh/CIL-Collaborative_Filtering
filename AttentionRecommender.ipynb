{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "id": "d1c1725ad3c6ea65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "NUM_SCIENTISTS = 10000\n",
    "NUM_PAPERS = 1000\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "# data\n",
    "SID_WISHLIST_SIZE = 34\n",
    "SID_CONTEXT_SIZE = 30\n",
    "\n",
    "PID_WISHLIST_SIZE = 50\n",
    "PID_CONTEXT_SIZE = 100\n",
    "\n",
    "\n",
    "# model\n",
    "EMBEDDING_DIM = 16\n",
    "DROPOUT_RATE = 0.3\n",
    "NUM_HEADS = 4\n",
    "\n",
    "# training\n",
    "L2_REG = 1e-4\n",
    "LEARNING_RATE= 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1e96234af4fac9da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATA_DIR = \"data/\"\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads in training data and splits it into training and validation sets with a 75/25 split.\n",
    "    Reads in wishlists and returns as data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "\n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25)\n",
    "\n",
    "    # read wishlist \n",
    "    tbr_df = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))\n",
    "\n",
    "    return train_df, valid_df, tbr_df\n",
    "\n",
    "def save_predictions_csv(model, train_loader, valid_loader, device, train_output_file: str, valid_output_file: str):\n",
    "    \"\"\"\n",
    "    Generates predictions from a trained model on both training and validation sets,\n",
    "    clamps and rounds the outputs to the nearest integer between 1 and 5,\n",
    "    and saves them as CSV files.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    def collect_predictions(loader):\n",
    "        \"\"\"Run inference and collect predictions for a given DataLoader.\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for sid, pid, rating, context, wishlist in loader:\n",
    "            # Move inputs to the target device\n",
    "            sid, pid, rating = sid.to(device), pid.to(device), rating.to(device)\n",
    "            context, wishlist = context.to(device), wishlist.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Run model prediction\n",
    "                predictions = model(sid, pid, context, wishlist)\n",
    "\n",
    "            # Collect results as list of dictionaries\n",
    "            for s, p, pred in zip(sid.cpu(), pid.cpu(), predictions.cpu()):\n",
    "                results.append({\n",
    "                    \"sid\": s.item(),\n",
    "                    \"pid\": p.item(),\n",
    "                    \"predicted\": pred.item()\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Generate and save predictions for training data\n",
    "    train_results = collect_predictions(train_loader)\n",
    "    pd.DataFrame(train_results).to_csv(train_output_file, index=False)\n",
    "    print(f\"Train predictions saved to {train_output_file}\")\n",
    "\n",
    "    # Generate and save predictions for validation data\n",
    "    valid_results = collect_predictions(valid_loader)\n",
    "    pd.DataFrame(valid_results).to_csv(valid_output_file, index=False)\n",
    "    print(f\"Validation predictions saved to {valid_output_file}\")"
   ],
   "id": "94509084f03452d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Build Datasets\n",
    "\n",
    "From each entry $(sid, pid, rating)$ in the original dataset, we generate two new entries: one for the **sid-dataset** and one for the **pid-dataset**. The **sid-dataset** entry is enriched by adding a **context** and a **wishlist** as follows:\n",
    "\n",
    "* **Context:**\n",
    "  The context contains up to `context_size` pairs $(pid, rating)$ representing papers that the same scientist ($sid$) has previously rated. These pairs are selected randomly. If there are fewer than `context_size` entries available, the remaining slots are filled with the padding value $(-1, 0)$.\n",
    "\n",
    "* **Wishlist:**\n",
    "  The wishlist includes up to `wishlist_size` paper IDs that are currently on the scientist’s ($sid$'s) wishlist. If there are not enough papers to fill the wishlist, the padding value $-1$ is used for the remaining entries.\n",
    "\n",
    "\n",
    "\n",
    "Similarly, it is done for the **pid-dataset**:\n",
    "\n",
    "* **Context:**\n",
    "  The context includes up to `context_size` pairs $(sid, rating)$, where each pair represents a scientist who has rated the paper `pid`. These are selected randomly from the available ratings. If there are fewer than `context_size` such entries, the padding value $(-1, 0)$ is used to fill the remaining slots.\n",
    "\n",
    "* **Wishlist:**\n",
    "  The wishlist consists of up to `wishlist_size` scientist IDs who have the paper `pid` on their wishlist. If there are not enough such scientists, the remaining entries are padded with the value $-1$.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "3a70ea64a44b6d6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_dataset_sid(df: pd.DataFrame, wishlist_df: pd.DataFrame, context_size: int, wishlist_size: int, save_path: str) -> torch.utils.data.Dataset:\n",
    "    \"\"\"\n",
    "    Constructs a PyTorch Dataset for each (sid, pid, rating) entry, enriched with:\n",
    "    - A fixed-size context of (pid, rating) tuples rated by the same scientist (sid).\n",
    "    - A fixed-size wishlist of paper IDs on the scientist's wishlist.\n",
    "\n",
    "    The dataset is cached at `save_path` if it exists.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading dataset from {save_path}\")\n",
    "        return torch.load(save_path)\n",
    "\n",
    "    # Extract data as tensors\n",
    "    sids = torch.from_numpy(df[\"sid\"].to_numpy())\n",
    "    pids = torch.from_numpy(df[\"pid\"].to_numpy())\n",
    "    ratings = torch.from_numpy(df[\"rating\"].to_numpy()).float()\n",
    "\n",
    "    # Build mappings for context and wishlist\n",
    "    sid_to_context = df.groupby(\"sid\")[[\"pid\", \"rating\"]].apply(\n",
    "        lambda x: list(zip(x[\"pid\"], x[\"rating\"]))\n",
    "    ).to_dict()\n",
    "\n",
    "    sid_to_wishlist = wishlist_df.groupby(\"sid\")[\"pid\"].apply(list).to_dict()\n",
    "\n",
    "    sid_context = []\n",
    "    sid_wishlist = []\n",
    "\n",
    "    # Construct dataset entries\n",
    "    for sid_val, pid_val in zip(sids, pids):\n",
    "        sid = sid_val.item()\n",
    "        pid = pid_val.item()\n",
    "\n",
    "        # Context: (pid, rating) tuples rated by this sid\n",
    "        context = [(p, r) for p, r in sid_to_context[sid] if p != pid]\n",
    "\n",
    "        # Pad or sample context to fixed size\n",
    "        if len(context) >= context_size:\n",
    "            context = random.sample(context, context_size)\n",
    "        else:\n",
    "            context += [(-1, 0.0)] * (context_size - len(context))\n",
    "\n",
    "        sid_context.append(torch.tensor(context, dtype=torch.int))\n",
    "\n",
    "        # Wishlist: papers on sid's wishlist\n",
    "        wishlist = sid_to_wishlist.get(sid, [])\n",
    "        if pid in wishlist:\n",
    "            wishlist.remove(pid)\n",
    "\n",
    "        if len(wishlist) >= wishlist_size:\n",
    "            wishlist = random.sample(wishlist, wishlist_size)\n",
    "        else:\n",
    "            wishlist += [-1] * (wishlist_size - len(wishlist))\n",
    "\n",
    "        sid_wishlist.append(torch.tensor(wishlist, dtype=torch.int))\n",
    "\n",
    "    # Stack all examples into tensors\n",
    "    sid_context = torch.stack(sid_context)\n",
    "    sid_wishlist = torch.stack(sid_wishlist)\n",
    "\n",
    "    # Create dataset and save\n",
    "    dataset = torch.utils.data.TensorDataset(sids, pids, ratings, sid_context, sid_wishlist)\n",
    "    torch.save(dataset, save_path)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_dataset_pid(df: pd.DataFrame, wishlist_df: pd.DataFrame, context_size: int, wishlist_size: int, save_path: str) -> torch.utils.data.Dataset:\n",
    "    \"\"\"\n",
    "    Constructs a PyTorch Dataset for each (sid, pid, rating) entry, enriched with:\n",
    "    - A fixed-size context of (sid, rating) tuples representing scientists who rated the same paper (pid).\n",
    "    - A fixed-size wishlist of sids who have the paper (pid) on their wishlist.\n",
    "\n",
    "    The dataset is cached at `save_path` if it exists.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading dataset from {save_path}\")\n",
    "        return torch.load(save_path)\n",
    "\n",
    "    # Extract data as tensors\n",
    "    sids = torch.from_numpy(df[\"sid\"].to_numpy())\n",
    "    pids = torch.from_numpy(df[\"pid\"].to_numpy())\n",
    "    ratings = torch.from_numpy(df[\"rating\"].to_numpy()).float()\n",
    "\n",
    "    # Build mappings for context and wishlist\n",
    "    pid_to_context = df.groupby(\"pid\")[[\"sid\", \"rating\"]].apply(\n",
    "        lambda x: list(zip(x[\"sid\"], x[\"rating\"]))\n",
    "    ).to_dict()\n",
    "\n",
    "    pid_to_wishlist = wishlist_df.groupby(\"pid\")[\"sid\"].apply(list).to_dict()\n",
    "\n",
    "    pid_context = []\n",
    "    pid_wishlist = []\n",
    "\n",
    "    # Construct dataset entries\n",
    "    for sid_val, pid_val in zip(sids, pids):\n",
    "        sid = sid_val.item()\n",
    "        pid = pid_val.item()\n",
    "\n",
    "        # Context: (sid, rating) tuples from other sids who rated the same pid\n",
    "        context = [(s, r) for s, r in pid_to_context[pid] if s != sid]\n",
    "\n",
    "        if len(context) >= context_size:\n",
    "            context = random.sample(context, context_size)\n",
    "        else:\n",
    "            context += [(-1, 0.0)] * (context_size - len(context))\n",
    "\n",
    "        pid_context.append(torch.tensor(context, dtype=torch.int))\n",
    "\n",
    "        # Wishlist: sids who wishlisted this pid\n",
    "        wishlist = pid_to_wishlist.get(pid, [])\n",
    "        if sid in wishlist:\n",
    "            wishlist.remove(sid)\n",
    "\n",
    "        if len(wishlist) >= wishlist_size:\n",
    "            wishlist = random.sample(wishlist, wishlist_size)\n",
    "        else:\n",
    "            wishlist += [-1] * (wishlist_size - len(wishlist))\n",
    "\n",
    "        pid_wishlist.append(torch.tensor(wishlist, dtype=torch.int))\n",
    "\n",
    "    # Stack all examples into tensors\n",
    "    pid_context = torch.stack(pid_context)\n",
    "    pid_wishlist = torch.stack(pid_wishlist)\n",
    "\n",
    "    # Create dataset and save\n",
    "    dataset = torch.utils.data.TensorDataset(sids, pids, ratings, pid_context, pid_wishlist)\n",
    "    torch.save(dataset, save_path)\n",
    "\n",
    "    return dataset\n"
   ],
   "id": "f19c896d21f54b99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Combining SID and PID Predictions\n",
    "\n",
    "This function constructs a PyTorch dataset by merging predictions from both the SID-based and PID-based models with the original ratings. Each entry in the resulting dataset contains:\n",
    "- `sid`: Scientist ID\n",
    "- `pid`: Paper ID\n",
    "- `rating`: Actual rating provided\n",
    "- `predicted_sid`: Prediction from the SID-based model\n",
    "- `predicted_pid`: Prediction from the PID-based model\n",
    "\n",
    "The final dataset is saved for efficient reuse.\n"
   ],
   "id": "c991dcb36629be7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_dataset_combined(rating_file: str, pred_sid_file: str, pred_pid_file: str, save_path: str) -> torch.utils.data.Dataset:\n",
    "    \"\"\"\n",
    "    Combines predictions from SID- and PID-based models with ground truth ratings\n",
    "    into a PyTorch TensorDataset and caches the result.\n",
    "    \"\"\"\n",
    "    full_save_path = os.path.join(DATA_DIR, save_path)\n",
    "\n",
    "    if os.path.exists(full_save_path):\n",
    "        print(f\"Loading dataset from {full_save_path}\")\n",
    "        return torch.load(full_save_path)\n",
    "\n",
    "    # Load CSVs\n",
    "    rating_df = pd.read_csv(os.path.join(DATA_DIR, rating_file))\n",
    "    pred_sid_df = pd.read_csv(os.path.join(DATA_DIR, pred_sid_file))\n",
    "    pred_pid_df = pd.read_csv(os.path.join(DATA_DIR, pred_pid_file))\n",
    "\n",
    "    # Extract 'sid' and 'pid' from 'sid_pid' column in rating_df\n",
    "    rating_df[['sid', 'pid']] = rating_df['sid_pid'].str.split(\"_\", expand=True)\n",
    "    rating_df = rating_df.drop(columns=[\"sid_pid\"])\n",
    "    rating_df[\"sid\"] = rating_df[\"sid\"].astype(int)\n",
    "    rating_df[\"pid\"] = rating_df[\"pid\"].astype(int)\n",
    "\n",
    "    # Merge prediction files on (sid, pid)\n",
    "    merged_df = pd.merge(pred_sid_df, pred_pid_df, on=[\"sid\", \"pid\"], suffixes=('_sid', '_pid'))\n",
    "\n",
    "    # Merge with actual ratings\n",
    "    merged_df = pd.merge(merged_df, rating_df, on=[\"sid\", \"pid\"])\n",
    "\n",
    "    # Convert to tensors\n",
    "    sids = torch.tensor(merged_df[\"sid\"].values, dtype=torch.int)\n",
    "    pids = torch.tensor(merged_df[\"pid\"].values, dtype=torch.int)\n",
    "    ratings = torch.tensor(merged_df[\"rating\"].values, dtype=torch.float)\n",
    "    preds_sid = torch.tensor(merged_df[\"predicted_sid\"].values, dtype=torch.float)\n",
    "    preds_pid = torch.tensor(merged_df[\"predicted_pid\"].values, dtype=torch.float)\n",
    "\n",
    "    # Create TensorDataset\n",
    "    dataset = torch.utils.data.TensorDataset(sids, pids, ratings, preds_sid, preds_pid)\n",
    "\n",
    "    # Save for reuse\n",
    "    torch.save(dataset, full_save_path)\n",
    "    print(f\"Combined dataset saved to {full_save_path}\")\n",
    "\n",
    "    return dataset\n"
   ],
   "id": "a56ba57245e56345",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attention-Based Recommender Models\n",
    "\n",
    "These two models implement attention-based collaborative filtering using both scientist (SID) and paper (PID) embeddings:\n",
    "\n",
    "- `AttentionRecommenderSID`: Focuses on attention over past paper ratings of each scientist.\n",
    "- `AttentionRecommenderPID`: Focuses on attention over scientist interactions for each paper.\n",
    "\n",
    "Each model:\n",
    "- Embeds users, items, and ratings.\n",
    "- Uses multi-head self-attention to integrate contextual and wishlist signals.\n",
    "- Applies a gating mechanism to combine multiple attention outputs.\n",
    "- Predicts a final rating score using a two-layer MLP head.\n"
   ],
   "id": "4f8dbca6651a7b39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AttentionRecommenderSID(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention-based recommender that focuses on scientist behavior (SID-centric).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_sids: int, num_pids: int, emb_dim: int, dropout_rate: float, num_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.sid_embedding = nn.Embedding(num_sids, emb_dim)\n",
    "        self.pid_embedding = nn.Embedding(num_pids + 1, emb_dim, padding_idx=-1)\n",
    "        self.rating_embedding = nn.Embedding(6, emb_dim, padding_idx=0)\n",
    "\n",
    "        # Layer norms for stabilization\n",
    "        self.norm_sid = nn.LayerNorm(emb_dim)\n",
    "        self.norm_pid = nn.LayerNorm(emb_dim)\n",
    "        self.norm_context_pid = nn.LayerNorm(emb_dim)\n",
    "        self.norm_rating = nn.LayerNorm(emb_dim)\n",
    "        self.output_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # Attention layers\n",
    "        self.context_attention = nn.MultiheadAttention(emb_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.wishlist_attention = nn.MultiheadAttention(emb_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "        # Projection layers\n",
    "        self.proj_attn_1 = nn.Linear(emb_dim, emb_dim)\n",
    "        self.proj_attn_2 = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Gating and final prediction layers\n",
    "        self.gate_layer = nn.Linear(2 * emb_dim, emb_dim)\n",
    "        self.fc1 = nn.Linear(emb_dim + 1, emb_dim)\n",
    "        self.fc2 = nn.Linear(emb_dim, 1)\n",
    "\n",
    "    def forward(self, sid, pid, context_pid_ratings, wishlist):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sid: (B,) \n",
    "            pid: (B,) \n",
    "            context_pid_ratings: (B, K, 2) where each entry is (pid, rating)\n",
    "            wishlist: (B, W) \n",
    "        Returns:\n",
    "            Predicted rating: (B,)\n",
    "        \"\"\"\n",
    "        context_pids = context_pid_ratings[:, :, 0].long()      # (B, K)\n",
    "        context_ratings = context_pid_ratings[:, :, 1].long()   # (B, K)\n",
    "\n",
    "        # Embedding lookups + normalization\n",
    "        sid_embed = self.norm_sid(self.sid_embedding(sid))      # (B, D)\n",
    "        pid_embed = self.norm_pid(self.pid_embedding(pid))      # (B, D)\n",
    "        wishlist_embed = self.norm_pid(self.pid_embedding(wishlist))    # (B, W, D)\n",
    "        context_pid_embed = self.norm_context_pid(self.pid_embedding(context_pids))         # (B, K, D)\n",
    "        context_rating_embed = self.norm_rating(self.rating_embedding(context_ratings))     # (B, K, D)\n",
    "\n",
    "        # Multi-head attention over context (with queries: pid + wishlist)\n",
    "        query = torch.cat([pid_embed.unsqueeze(1), wishlist_embed], dim=1)      # (B, W+1, D)\n",
    "        context_attn, _ = self.context_attention(query, context_pid_embed, context_rating_embed)    # (B, W+1, D)\n",
    "        context_attn = self.output_norm(self.proj_attn_1(context_attn))         # (B, W+1, D)\n",
    "\n",
    "        rating_pid = context_attn[:, 0, :]          # (B, D)\n",
    "        ratings_wishlist = context_attn[:, 1:, :]   # (B, W, D)\n",
    "\n",
    "        # Attention over wishlist\n",
    "        rating_wishlist, _ = self.wishlist_attention(pid_embed.unsqueeze(1), wishlist_embed, ratings_wishlist)  # (B, 1, D)\n",
    "        rating_wishlist = self.proj_attn_2(rating_wishlist.squeeze(1))      # (B, D)\n",
    "\n",
    "        # Fusion via gating\n",
    "        gate = torch.sigmoid(self.gate_layer(torch.cat([sid_embed, pid_embed], dim=1)))\n",
    "        fused = gate * rating_pid + (1 - gate) * rating_wishlist        # (B, D)\n",
    "\n",
    "        bias = torch.sum(sid_embed * pid_embed, dim=-1, keepdim=True)   # (B,)\n",
    "\n",
    "        x = torch.cat([bias, fused], dim=1)         # (B, D+1)\n",
    "        x = F.relu(self.fc1(self.dropout(x)))       # (B, D)\n",
    "        x = self.fc2(self.dropout(x)).squeeze(1)    # (B,)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionRecommenderPID(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention-based recommender that focuses on product behavior (PID-centric).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_sids: int, num_pids: int, emb_dim: int, dropout_rate: float, num_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.sid_embedding = nn.Embedding(num_sids + 1, emb_dim, padding_idx=-1)\n",
    "        self.pid_embedding = nn.Embedding(num_pids, emb_dim)\n",
    "        self.rating_embedding = nn.Embedding(6, emb_dim, padding_idx=0)\n",
    "\n",
    "        # Layer norms for stabilization\n",
    "        self.norm_sid = nn.LayerNorm(emb_dim)\n",
    "        self.norm_pid = nn.LayerNorm(emb_dim)\n",
    "        self.norm_context_sid = nn.LayerNorm(emb_dim)\n",
    "        self.norm_rating = nn.LayerNorm(emb_dim)\n",
    "        self.output_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # Attention layers\n",
    "        self.context_attention = nn.MultiheadAttention(emb_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.wishlist_attention = nn.MultiheadAttention(emb_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "        # Projection layers\n",
    "        self.proj_attn_1 = nn.Linear(emb_dim, emb_dim)\n",
    "        self.proj_attn_2 = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Gating and final prediction layers\n",
    "        self.gate_layer = nn.Linear(2 * emb_dim, emb_dim)\n",
    "        self.fc1 = nn.Linear(emb_dim + 1, emb_dim)\n",
    "        self.fc2 = nn.Linear(emb_dim, 1)\n",
    "\n",
    "    def forward(self, sid, pid, context_pid_ratings, wishlist):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sid: (B,)\n",
    "            pid: (B,)\n",
    "            context_pid_ratings: (B, K, 2) where each entry is (sid, rating)\n",
    "            wishlist: (B, W) \n",
    "        Returns:\n",
    "            Predicted rating: (B,)\n",
    "        \"\"\"\n",
    "        context_sids = context_pid_ratings[:, :, 0].long()      # (B, K)\n",
    "        context_ratings = context_pid_ratings[:, :, 1].long()   # (B, K)\n",
    "\n",
    "        # Embedding lookups + normalization\n",
    "        sid_embed = self.norm_sid(self.sid_embedding(sid))      # (B, D)\n",
    "        pid_embed = self.norm_pid(self.pid_embedding(pid))      # (B, D)\n",
    "        wishlist_embed = self.norm_sid(self.sid_embedding(wishlist))    # (B, W, D)\n",
    "        context_sid_embed = self.norm_context_sid(self.sid_embedding(context_sids))         # (B, K, D)\n",
    "        context_rating_embed = self.norm_rating(self.rating_embedding(context_ratings))     # (B, K, D)\n",
    "\n",
    "        # Multi-head attention over context (with queries: pid + wishlist)\n",
    "        query = torch.cat([sid_embed.unsqueeze(1), wishlist_embed], dim=1)      # (B, W+1, D)\n",
    "        context_attn, _ = self.context_attention(query, context_sid_embed, context_rating_embed)    # (B, W+1, D)\n",
    "        context_attn = self.output_norm(self.proj_attn_1(context_attn))         # (B, W+1, D)\n",
    "\n",
    "        rating_sid = context_attn[:, 0, :]          # (B, D)\n",
    "        ratings_wishlist = context_attn[:, 1:, :]   # (B, W, D)\n",
    "\n",
    "        # Attention over wishlist\n",
    "        rating_wishlist, _ = self.wishlist_attention(sid_embed.unsqueeze(1), wishlist_embed, ratings_wishlist)  # (B, 1, D)\n",
    "        rating_wishlist = self.proj_attn_2(rating_wishlist.squeeze(1))      # (B, D)\n",
    "\n",
    "        # Fusion via gating\n",
    "        gate = torch.sigmoid(self.gate_layer(torch.cat([sid_embed, pid_embed], dim=1)))\n",
    "        fused = gate * rating_sid + (1 - gate) * rating_wishlist    # (B, D)\n",
    "\n",
    "        bias = torch.sum(sid_embed * pid_embed, dim=-1, keepdim=True)   # (B,)\n",
    "        \n",
    "        x = torch.cat([bias, fused], dim=1)         # (B, D+1)\n",
    "        x = F.relu(self.fc1(self.dropout(x)))       # (B, D)\n",
    "        x = self.fc2(self.dropout(x)).squeeze(1)    # (B,)\n",
    "\n",
    "        return x\n"
   ],
   "id": "cb0b45ba7c32b15e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model(model, optim, device, epochs, train_loader, valid_loader):\n",
    "    \"\"\"\n",
    "    Trains the model using MSE loss and evaluates it on validation data\n",
    "    after each epoch. Outputs training loss and validation RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_data = 0\n",
    "\n",
    "        # Training loop\n",
    "        for sid, pid, rating, context, wishlist in train_loader:\n",
    "            # Move data to the target device\n",
    "            sid, pid, rating = sid.to(device), pid.to(device), rating.to(device)\n",
    "            context, wishlist = context.to(device), wishlist.to(device)\n",
    "\n",
    "            # Forward pass and compute loss\n",
    "            predictions = model(sid, pid, context, wishlist)\n",
    "            loss = F.mse_loss(predictions, rating)\n",
    "\n",
    "            # Backpropagation\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item() * sid.size(0)\n",
    "            total_data += sid.size(0)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_val_data = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sid, pid, rating, context, wishlist in valid_loader:\n",
    "                sid, pid, rating = sid.to(device), pid.to(device), rating.to(device)\n",
    "                context, wishlist = context.to(device), wishlist.to(device)\n",
    "\n",
    "                predictions = model(sid, pid, context, wishlist)\n",
    "                mse = F.mse_loss(predictions, rating)\n",
    "\n",
    "                total_val_loss += mse.item() * sid.size(0)\n",
    "                total_val_data += sid.size(0)\n",
    "\n",
    "        # Calculate training and validation RMSE\n",
    "        train_rmse = (total_loss / total_data) ** 0.5\n",
    "        val_rmse = (total_val_loss / total_val_data) ** 0.5\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}/{epochs}] Train RMSE={train_rmse:.3f}, Valid RMSE={val_rmse:.3f}\")\n"
   ],
   "id": "330bd1c126e3a8ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`CrossLayer`: A single layer of feature crossing, based on:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{l+1} = \\mathbf{x}_0 \\cdot (\\mathbf{w}^T \\mathbf{x}_l) + \\mathbf{b} + \\mathbf{x}_l\n",
    "$$\n",
    "\n",
    "* Captures pairwise feature interactions.\n",
    "* Used in sequence to model explicit cross terms.\n",
    "\n",
    "\n",
    "`RecommenderFinal`: It combines: \n",
    "* **Embeddings** for `sid` and `pid`.\n",
    "* **Cross Layers** for explicit feature interactions.\n",
    "* **Deep MLP** for non-linear modeling.\n",
    "\n",
    "As inputs it uses the predictions of the previous two attention based recommenders.\n",
    "* **Inputs**: `sid`, `pid`, `pred_sid`, `pred_pid`\n",
    "* **Output**: Rating prediction ∈ \\[1, 5]\n"
   ],
   "id": "b5e6970bca8ba373"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CrossLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # Trainable parameters for the cross layer\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim))  # (D,)\n",
    "        self.bias = nn.Parameter(torch.randn(input_dim))    # (D,)\n",
    "\n",
    "    def forward(self, x0, x):\n",
    "        \"\"\"\n",
    "        Cross interaction layer:\n",
    "        x_{l+1} = x0 * (w^T x) + b + x\n",
    "        \"\"\"\n",
    "        xw = torch.sum(x * self.weight, dim=1, keepdim=True)  # (B, 1)\n",
    "        out = x0 * xw + self.bias + x    # (B, D)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RecommenderFinal(nn.Module):\n",
    "    def __init__(self, num_sids, num_pids, emb_dim, hidden_dim, num_cross_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers for SID and PID\n",
    "        self.sid_embedding = nn.Embedding(num_sids, emb_dim)\n",
    "        self.pid_embedding = nn.Embedding(num_pids, emb_dim)\n",
    "\n",
    "        # Input features: sid_emb + pid_emb + pred_sid + pred_pid\n",
    "        input_dim = 2 * emb_dim + 2\n",
    "\n",
    "        # Cross layers to model explicit feature interactions\n",
    "        self.cross_layers = nn.ModuleList([\n",
    "            CrossLayer(input_dim) for _ in range(num_cross_layers)\n",
    "        ])\n",
    "\n",
    "        # DNN\n",
    "        self.deep = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)  # Final regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, sid, pid, pred_sid, pred_pid):\n",
    "        \"\"\"\n",
    "        Forward pass through the final hybrid recommender.\n",
    "        \"\"\"\n",
    "        sid_emb = self.sid_embedding(sid)  # (B, D)\n",
    "        pid_emb = self.pid_embedding(pid)  # (B, D)\n",
    "\n",
    "        # Concatenate raw features and intermediate predictions\n",
    "        features = torch.cat([sid_emb, pid_emb, pred_sid.unsqueeze(1), pred_pid.unsqueeze(1)], dim=1)  # (B, 2D+2)\n",
    "\n",
    "        # Pass through Cross Network\n",
    "        x = features\n",
    "        for layer in self.cross_layers:\n",
    "            x = layer(features, x)\n",
    "\n",
    "        # Pass through DNN\n",
    "        out = self.deep(x).squeeze(-1)\n",
    "\n",
    "        # Final output clamped to rating range\n",
    "        out = torch.clamp(out, 1, 5)\n",
    "\n",
    "        return out\n"
   ],
   "id": "728dcd891f3f95be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model instantiation and training",
   "id": "bd2b34de7656987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "model_sid = AttentionRecommenderSID(num_sids=NUM_SCIENTISTS, num_pids=NUM_PAPERS, emb_dim=EMBEDDING_DIM, dropout_rate=DROPOUT_RATE, num_heads=NUM_HEADS).to(device)\n",
    "optim_sid = torch.optim.Adam(model_sid.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)\n",
    "\n",
    "model_pid = AttentionRecommenderPID(num_sids=NUM_SCIENTISTS, num_pids=NUM_PAPERS, emb_dim=EMBEDDING_DIM, dropout_rate=DROPOUT_RATE, num_heads=NUM_HEADS).to(device)\n",
    "optim_pid = torch.optim.Adam(model_pid.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)"
   ],
   "id": "516f8ddcaa0fb72c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df, valid_df, tbr_df = read_data_df()",
   "id": "5be1dfd6c0d79685",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset_sid = get_dataset_sid(df=train_df, wishlist_df=tbr_df, context_size=SID_CONTEXT_SIZE, wishlist_size=SID_WISHLIST_SIZE, save_path=\"data/sid_train_dataset\")\n",
    "valid_dataset_sid = get_dataset_sid(df=valid_df, wishlist_df=tbr_df, context_size=SID_CONTEXT_SIZE, wishlist_size=SID_WISHLIST_SIZE, save_path=\"data/sid_valid_dataset\")\n",
    "\n",
    "train_loader_sid = torch.utils.data.DataLoader(train_dataset_sid, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader_sid = torch.utils.data.DataLoader(valid_dataset_sid, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "train_dataset_pid = get_dataset_pid(df=train_df, wishlist_df=tbr_df, context_size=PID_CONTEXT_SIZE, wishlist_size=PID_WISHLIST_SIZE, save_path=\"data/pid_train_dataset\")\n",
    "valid_dataset_pid = get_dataset_pid(df=valid_df, wishlist_df=tbr_df, context_size=PID_CONTEXT_SIZE, wishlist_size=PID_WISHLIST_SIZE, save_path=\"data/pid_valid_dataset\")\n",
    "\n",
    "train_loader_pid = torch.utils.data.DataLoader(train_dataset_pid, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader_pid = torch.utils.data.DataLoader(valid_dataset_pid, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "2e45c83459b9b1fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_model(model_sid, optim_sid, device, EPOCHS, train_loader_sid, valid_loader_sid)\n",
    "torch.save(model_sid.state_dict(), 'attention_recommender_sid.pt')\n",
    "\n",
    "save_predictions_csv(model_sid, train_loader_sid, valid_loader_sid, device, train_output_file=\"data/sid_train_predictions.csv\", valid_output_file=\"data/sid_valid_predictions.csv\")"
   ],
   "id": "a5262d76fc52afb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_model(model_pid, optim_pid, device, EPOCHS, train_loader_pid, valid_loader_pid)\n",
    "torch.save(model_pid.state_dict(), 'attention_recommender_pid.pt')\n",
    "\n",
    "save_predictions_csv(model_pid, train_loader_pid, valid_loader_pid, device, train_output_file=\"data/pid_train_predictions.csv\", valid_output_file=\"data/pid_valid_predictions.csv\")"
   ],
   "id": "5f588a7d8ac84eab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset_combined = get_dataset_combined(rating_file=\"train_ratings.csv\", pred_sid_file=\"sid_train_predictions.csv\", pred_pid_file=\"pid_train_predictions.csv\", save_path=\"combined_train_dataset\")\n",
    "valid_dataset_combined = get_dataset_combined(rating_file=\"train_ratings.csv\", pred_sid_file=\"sid_valid_predictions.csv\", pred_pid_file=\"pid_valid_predictions.csv\", save_path=\"combined_valid_dataset\")\n",
    "\n",
    "train_loader_combined = torch.utils.data.DataLoader(train_dataset_combined, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader_combined = torch.utils.data.DataLoader(valid_dataset_combined, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "2a2bb583fbe5ac24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_combined = RecommenderFinal(num_sids=NUM_SCIENTISTS, num_pids=NUM_PAPERS, emb_dim=64, hidden_dim=128, num_cross_layers=2, dropout_rate=0.2).to(device)\n",
    "optim_combined = torch.optim.Adam(model_combined.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)"
   ],
   "id": "17aee5f8e3791ebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train model for an epoch\n",
    "    total_loss = 0.0\n",
    "    total_data = 0\n",
    "    model_combined.train()\n",
    "    for sid, pid, rating, pred_sid, pred_pid in train_loader_combined:\n",
    "        # Move data to GPU\n",
    "        sid, pid, rating = sid.to(device), pid.to(device), rating.to(device)\n",
    "        pred_sid, pred_pid = pred_sid.to(device), pred_pid.to(device)\n",
    "\n",
    "        # Make prediction and compute loss\n",
    "        pred = model_combined(sid, pid, pred_sid, pred_pid)        \n",
    "        loss = F.mse_loss(pred, rating)\n",
    "\n",
    "        # Compute gradients w.r.t. loss and take a step in that direction\n",
    "        optim_combined.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_combined.step()\n",
    "\n",
    "        # Keep track of running loss\n",
    "        total_data += len(sid)\n",
    "        total_loss += len(sid) * loss.item()\n",
    "\n",
    "    # Evaluate model on validation data\n",
    "    total_val_mse = 0.0\n",
    "    total_val_data = 0\n",
    "\n",
    "    model_combined.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sid, pid, rating, pred_sid, pred_pid in valid_loader_combined:\n",
    "            # Move data to GPU\n",
    "            sid, pid, rating = sid.to(device), pid.to(device), rating.to(device)\n",
    "            pred_sid, pred_pid = pred_sid.to(device), pred_pid.to(device)\n",
    "\n",
    "            pred = model_combined(sid, pid, pred_sid, pred_pid)\n",
    "            mse = F.mse_loss(pred, rating)\n",
    "\n",
    "            # Keep track of running metrics\n",
    "            total_val_data += len(sid)\n",
    "            total_val_mse += len(sid) * mse.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{EPOCHS}] Train loss={total_loss / total_data:.3f}, Valid RMSE={(total_val_mse / total_val_data) ** 0.5:.3f}\")"
   ],
   "id": "6e28406474d0c456",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
