{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:27:19.079708Z",
     "start_time": "2025-05-20T10:27:15.122197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.rcsetup import validate_markevery\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import os\n",
    "import random\n",
    "from sympy.physics.quantum.identitysearch import scipy\n"
   ],
   "id": "e2d75465dc8a03d7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:27:19.093986Z",
     "start_time": "2025-05-20T10:27:19.089742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ],
   "id": "4bb99a3c676cbcac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper functions",
   "id": "a64a1ed68e201a97"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T10:27:19.102454Z",
     "start_time": "2025-05-20T10:27:19.098089Z"
    }
   },
   "source": [
    "#DATA_DIR = \"/cluster/courses/cil/collaborative_filtering/data\"\n",
    "DATA_DIR = \"\"\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"data/train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "\n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25)\n",
    "    return train_df, valid_df\n",
    "\n",
    "\n",
    "def read_data_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Returns matrix view of the training data, where columns are scientists (sid) and\n",
    "    rows are papers (pid).\"\"\"\n",
    "\n",
    "    return df.pivot(index=\"sid\", columns=\"pid\", values=\"rating\").values\n",
    "\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "\n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "\n",
    "def make_submission(pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray], filename: os.PathLike):\n",
    "    \"\"\"Makes a submission CSV file that can be submitted to kaggle.\n",
    "\n",
    "    Inputs:\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs a score.\n",
    "        filename: File to save the submission to.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "    # Get sids and pids\n",
    "    sid_pid = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids = sid_pid[0]\n",
    "    pids = sid_pid[1]\n",
    "    sids = sids.astype(int).values\n",
    "    pids = pids.astype(int).values\n",
    "\n",
    "    df[\"rating\"] = pred_fn(sids, pids)\n",
    "    df.to_csv(filename, index=False)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "For the first method in this introduction, we will make use of the singular value decomposition (SVD) to construct the optimal rank-$k$ approximation (when measuring the Frobenius norm as error), according to the Eckart-Young theorem. Since the matrix needs to be fully observed in order to make use of SVD, we need to impute the missing values. In this case, we impute values with $3$."
   ],
   "id": "e235d3dcde6aa4fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T11:03:15.917635Z",
     "start_time": "2025-05-21T11:03:15.914564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def opt_rank_k_approximation(m: np.ndarray, k: int):\n",
    "    \"\"\"Returns the optimal rank-k reconstruction matrix, using SVD.\"\"\"\n",
    "\n",
    "    assert 0 < k <= np.min(m.shape), f\"The rank must be in [0, min(m, n)]\"\n",
    "\n",
    "    U, S, Vh = np.linalg.svd(m, full_matrices=False)\n",
    "\n",
    "    U_k = U[:, :k]\n",
    "    S_k = S[:k]\n",
    "    Vh_k = Vh[:k]\n",
    "\n",
    "    return np.dot(U_k * S_k, Vh_k)\n",
    "\n",
    "\n",
    "def matrix_pred_fn(train_recon: np.ndarray, sids: np.ndarray, pids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        train_recon: (M, N) matrix with predicted values for every (sid, pid) pair.\n",
    "        sids: (D,) vector with integer scientist IDs.\n",
    "        pids: (D,) vector with integer paper IDs.\n",
    "        \n",
    "    Outputs: (D,) vector with predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    return train_recon[sids, pids]"
   ],
   "id": "3ec0d06059df1a57",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T11:06:08.940637Z",
     "start_time": "2025-05-21T11:06:03.872909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def impute_int(matrix):\n",
    "    matrix_filled = matrix.copy()\n",
    "    return np.nan_to_num(matrix_filled, nan=3)\n",
    "\n",
    "def impute_mean(matrix):\n",
    "    matrix_filled = matrix.copy()\n",
    "    for i in range(matrix.shape[0]):\n",
    "        row_means = (matrix_filled[i][~np.isnan(matrix[i])]).mean()\n",
    "        matrix_filled[i][np.isnan(matrix[i])] = row_means\n",
    "    return matrix_filled\n",
    "\n",
    "def impute_global_mean(matrix):\n",
    "    global_mean = matrix[~np.isnan(matrix)].mean()\n",
    "    matrix_filled = matrix.copy()\n",
    "    matrix_filled[np.isnan(matrix)] = global_mean\n",
    "    return matrix_filled\n",
    "\n",
    "# Assuming read_data_df(), read_data_matrix(), evaluate() are defined elsewhere\n",
    "train_df, valid_df = read_data_df()\n",
    "train_mat_original = read_data_matrix(train_df)\n",
    "\n",
    "def run_experiment(impute_fn, impute_name):\n",
    "    train_mat = impute_fn(train_mat_original)\n",
    "    train_recon = opt_rank_k_approximation(train_mat, k=2)\n",
    "    pred_fn = lambda sids, pids: matrix_pred_fn(train_recon, sids, pids)\n",
    "    \n",
    "    test_score = evaluate(train_df, pred_fn)\n",
    "    val_score = evaluate(valid_df, pred_fn)\n",
    "    print(f\"{impute_name} | Train RMSE: {test_score:.4f}, Validation RMSE: {val_score:.4f}\")\n",
    "\n",
    "run_experiment(impute_int, \"Impute with 3\")\n",
    "run_experiment(impute_mean, \"Impute with Row Mean\")\n",
    "run_experiment(impute_global_mean, \"Impute with Global Mean\")\n"
   ],
   "id": "34b4e51a3fcfa2d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impute with 3 | Train RMSE: 1.2005, Validation RMSE: 1.2068\n",
      "Impute with Row Mean | Train RMSE: 0.9152, Validation RMSE: 0.9344\n",
      "Impute with Global Mean | Train RMSE: 0.9781, Validation RMSE: 0.9876\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ALS",
   "id": "aa74c2011cf6a29e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T18:12:34.708209Z",
     "start_time": "2025-05-21T18:12:32.847963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "factor = 10\n",
    "lambda_reg = 1\n",
    "num_iterations = 10\n",
    "\n",
    "train_df, valid_df = read_data_df()\n",
    "train_mat = read_data_matrix(train_df) # num_pid x num_sid\n",
    "\n",
    "num_pid = train_mat.shape[0]\n",
    "num_sid = train_mat.shape[1]\n",
    "\n",
    "U = np.random.normal(size=(num_pid, factor))\n",
    "V = np.random.normal(size=(num_sid, factor))\n",
    "\n",
    "mask = ~np.isnan(train_mat)"
   ],
   "id": "fef25693676ecefe",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Updating the two Matrices $U$ and $V$ alternately, i.e.\n",
    "\n",
    "$$u_i = \\left(\\sum_{j}\\omega_{ij}\\cdot v_j v_j^T + \\lambda \\cdot I\\right)^{-1}\\left(\\sum_j\\omega_{ij} \\cdot a_{ij}v_j\\right)$$\n",
    "and \n",
    "$$v_j = \\left(\\sum_{i}\\omega_{ij}\\cdot u_iu_i^T + \\lambda \\cdot I\\right)^{-1}\\left(\\sum_i\\omega_{ij} \\cdot a_{ij}u_i\\right)$$"
   ],
   "id": "631a58c5343cf7ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T18:12:39.519318Z",
     "start_time": "2025-05-21T18:12:36.020788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for it in range(num_iterations):\n",
    "    # Fix V, solve for U\n",
    "    for i in range(num_pid):\n",
    "        V_i = V[mask[i]]\n",
    "        train_mat_i = train_mat[i][mask[i]]\n",
    "        A = V_i.T @ V_i + lambda_reg * np.eye(factor)\n",
    "        b = V_i.T @ train_mat_i\n",
    "        U[i] = np.linalg.solve(A, b)\n",
    "\n",
    "    # Fix U, solve for V\n",
    "    for j in range(num_sid):\n",
    "        U_j = U[mask[:, j]]\n",
    "        train_mat_j = train_mat[:, j][mask[:, j]]\n",
    "        A = U_j.T @ U_j + lambda_reg * np.eye(factor)\n",
    "        b = U_j.T @ train_mat_j\n",
    "        V[j] = np.linalg.solve(A, b)\n",
    "\n",
    "    # Compute loss\n",
    "    recon = U @ V.T\n",
    "    squared_error = ((train_mat - recon)[mask])**2\n",
    "    mse = np.sum(squared_error) / np.sum(mask)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    pred_fn = lambda sids, pids: matrix_pred_fn(recon, sids, pids)\n",
    "    test_score = evaluate(train_df, pred_fn)\n",
    "    val_score = evaluate(valid_df, pred_fn)\n",
    "\n",
    "    print(f\"Iteration {it + 1}/{num_iterations}, RMSE: {test_score:.4f}, Validation RMSE: {val_score:.4f}\")"
   ],
   "id": "ccdcdf07d362b45e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, RMSE: 2.5229, Validation RMSE: 2.7304\n",
      "Iteration 2/10, RMSE: 0.8023, Validation RMSE: 0.9174\n",
      "Iteration 3/10, RMSE: 0.7774, Validation RMSE: 0.9119\n",
      "Iteration 4/10, RMSE: 0.7659, Validation RMSE: 0.9079\n",
      "Iteration 5/10, RMSE: 0.7596, Validation RMSE: 0.9053\n",
      "Iteration 6/10, RMSE: 0.7559, Validation RMSE: 0.9039\n",
      "Iteration 7/10, RMSE: 0.7536, Validation RMSE: 0.9032\n",
      "Iteration 8/10, RMSE: 0.7521, Validation RMSE: 0.9029\n",
      "Iteration 9/10, RMSE: 0.7510, Validation RMSE: 0.9028\n",
      "Iteration 10/10, RMSE: 0.7502, Validation RMSE: 0.9028\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Variational Autoencoder\n",
    "\n",
    "A **Variational Autoencoder (VAE)** is a generative model that learns to encode input data into a **latent space** represented by a probability distribution. Instead of mapping inputs to fixed points, the encoder outputs parameters (mean and variance) of this distribution. During training, latent vectors are sampled from these distributions, allowing the decoder to reconstruct the input. "
   ],
   "id": "74eca3cd579b3867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T10:47:51.798252Z",
     "start_time": "2025-05-21T10:47:51.792291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, rating_matrix, impute_func):\n",
    "        self.original_data = rating_matrix\n",
    "        self.data = impute_func(rating_matrix)\n",
    "        self.mask = ~torch.isnan(torch.tensor(rating_matrix)).to(torch.bool) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.mask[idx]\n",
    "\n",
    "\n",
    "class VAE_CF(nn.Module):\n",
    "    def __init__(self, num_items, hidden_dim=20, latent_dim=10, dropout=0.5):\n",
    "        super(VAE_CF, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_items, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_items)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar, mask):\n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.mse_loss(recon_x[mask], x[mask])\n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + kl_loss, recon_loss.item(), kl_loss\n"
   ],
   "id": "536b0d9654fcb4d2",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define imputation strategies\n",
    "def impute_int(matrix):\n",
    "    matrix_filled = matrix.copy()\n",
    "    return torch.tensor(np.nan_to_num(matrix_filled, nan=3), dtype=torch.float32)\n",
    "\n",
    "def impute_mean(matrix):    \n",
    "    matrix_filled = matrix.copy()\n",
    "    for i in range(matrix.shape[0]):\n",
    "        row_means = (matrix_filled[i][~np.isnan(matrix[i])]).mean()\n",
    "        matrix_filled[i][np.isnan(matrix[i])] = row_means\n",
    "    return torch.tensor(matrix_filled, dtype=torch.float32)\n",
    "\n",
    "def impute_global_mean(matrix):\n",
    "    global_mean = matrix[~np.isnan(matrix)].mean()\n",
    "    matrix_filled = matrix.copy()\n",
    "    matrix_filled[np.isnan(matrix)] = global_mean\n",
    "    return torch.tensor(matrix_filled, dtype=torch.float32)\n",
    "\n",
    "# Load data\n",
    "train_df, valid_df = read_data_df()\n",
    "train_mat = read_data_matrix(train_df)\n",
    "valid_mat = read_data_matrix(valid_df)\n",
    "\n",
    "# Define imputation strategies to test\n",
    "imputation_strategies = {\n",
    "    \"int (3)\": impute_int,\n",
    "    \"row_mean\": impute_mean,\n",
    "    \"global_mean\": impute_global_mean,\n",
    "}\n",
    "\n",
    "# Evaluate each strategy\n",
    "for name, impute_func in imputation_strategies.items():\n",
    "    print(f\"\\n>>> Imputation strategy: {name}\")\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = RatingDataset(train_filled, impute_func)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = VAE_CF(num_items=train_mat.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_data = 0\n",
    "        for batch, batch_mask in train_loader:\n",
    "            recon, mu, logvar = model(batch)\n",
    "            loss, recon_loss, kl = model.loss_function(recon, batch, mu, logvar, batch_mask)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += recon_loss * len(batch)\n",
    "            total_data += len(batch)\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use imputed train matrix to get representations\n",
    "            train_tensor = impute_func(train_mat)\n",
    "            recon, _, _ = model(train_tensor)\n",
    "    \n",
    "            # Evaluate only on valid entries in validation matrix\n",
    "            valid_mask = ~np.isnan(valid_mat)\n",
    "            valid_true = torch.tensor(valid_mat[valid_mask], dtype=torch.float32)\n",
    "            valid_pred = recon[valid_mask]\n",
    "    \n",
    "            val_rmse = torch.sqrt(torch.mean((valid_true - valid_pred) ** 2)).item()\n",
    "            print(f\"Epoch {epoch+1:02d} | Train RMSE: {(total_loss / total_data) ** 0.5:.4f}, Validation RMSE: {val_rmse:.4f}\")"
   ],
   "id": "baa2d1009158e17e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Learned embeddings\n",
    "\n",
    "Next, we will take a machine learning view of the problem. To each scientist and paper, we assign a $d$-dimensional embedding and we predict the rating that the scientist gives the paper to be their dot product. More formally, let $\\vec{s}_i$ be a scientist embedding and $\\vec{p}_j$ be a paper embedding. Then, we make the following rating prediction for this pair: $$\\tilde{r}_{ij} = \\langle \\vec{s}_i, \\vec{p}_j \\rangle.$$ We view these embeddings as our learnable parameters and train them as we would any other model using the squared error loss function: $$\\ell(\\theta) = \\frac{1}{2} |\\langle \\vec{s}_i, \\vec{p}_j \\rangle - r_{ij}|^2,$$ where $\\theta = \\{ \\vec{s}_i \\}_{i=1}^n \\cup \\{ \\vec{p}_j \\}_{j=1}^m$. The following is an implementation of this method.## Embedding"
   ],
   "id": "8693228ca4a2087a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T07:37:07.727991Z",
     "start_time": "2025-05-22T07:37:07.722076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dataset(df: pd.DataFrame) -> torch.utils.data.Dataset:\n",
    "    \"\"\"Conversion from pandas data frame to torch dataset.\"\"\"\n",
    "\n",
    "    sids = torch.from_numpy(df[\"sid\"].to_numpy())\n",
    "    pids = torch.from_numpy(df[\"pid\"].to_numpy())\n",
    "    ratings = torch.from_numpy(df[\"rating\"].to_numpy()).float()\n",
    "    return torch.utils.data.TensorDataset(sids, pids, ratings)"
   ],
   "id": "5a381eed59ab44de",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T07:37:08.840080Z",
     "start_time": "2025-05-22T07:37:08.836700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EmbeddingDotProductModel(nn.Module):\n",
    "    def __init__(self, num_scientists: int, num_papers: int, dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assign to each scientist and paper an embedding\n",
    "        self.scientist_emb = nn.Embedding(num_scientists, dim)\n",
    "        self.paper_emb = nn.Embedding(num_papers, dim)\n",
    "\n",
    "    def forward(self, sid: torch.Tensor, pid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            sid: [B,], int\n",
    "            pid: [B,], int\n",
    "        \n",
    "        Outputs: [B,], float\n",
    "        \"\"\"\n",
    "\n",
    "        # Per-pair dot product\n",
    "        return torch.sum(self.scientist_emb(sid) * self.paper_emb(pid), dim=-1)"
   ],
   "id": "7876fdfa7726f797",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T07:37:10.597895Z",
     "start_time": "2025-05-22T07:37:10.581690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define model (10k scientists, 1k papers, 32-dimensional embeddings) and optimizer\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model = EmbeddingDotProductModel(10_000, 1_000, 32).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "id": "5e4615d8b4c06e69",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T07:37:12.247531Z",
     "start_time": "2025-05-22T07:37:12.237041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = get_dataset(train_df)\n",
    "valid_dataset = get_dataset(valid_df)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "80078dcff3ceb7da",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T08:15:47.991078Z",
     "start_time": "2025-05-22T08:12:53.632833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_EPOCHS = 5\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train model for an epoch\n",
    "    total_loss = 0.0\n",
    "    total_data = 0\n",
    "    model.train()\n",
    "    for sid, pid, ratings in train_loader:\n",
    "        sid = sid.to(device)\n",
    "        pid = pid.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        # Make prediction and compute loss\n",
    "        pred = model(sid, pid)\n",
    "        loss = F.mse_loss(pred, ratings)\n",
    "\n",
    "        # Compute gradients w.r.t. loss and take a step in that direction\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Keep track of running loss\n",
    "        total_data += len(sid)\n",
    "        total_loss += len(sid) * loss.item()\n",
    "\n",
    "    # Evaluate model on validation data\n",
    "    total_val_mse = 0.0\n",
    "    total_val_data = 0\n",
    "    model.eval()\n",
    "    for sid, pid, ratings in valid_loader:\n",
    "        # Move data to GPU\n",
    "        sid = sid.to(device)\n",
    "        pid = pid.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        # Clamp predictions in [1,5], since all ground-truth ratings are\n",
    "        pred = model(sid, pid).clamp(1, 5)\n",
    "        mse = F.mse_loss(pred, ratings)\n",
    "\n",
    "        # Keep track of running metrics\n",
    "        total_val_data += len(sid)\n",
    "        total_val_mse += len(sid) * mse.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Train loss={total_loss / total_data:.3f}, Valid RMSE={(total_val_mse / total_val_data) ** 0.5:.3f}\")"
   ],
   "id": "2fe7e6bebfc5b74b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] Train loss=0.849, Valid RMSE=0.949\n",
      "[Epoch 2/5] Train loss=0.828, Valid RMSE=0.946\n",
      "[Epoch 3/5] Train loss=0.803, Valid RMSE=0.943\n",
      "[Epoch 4/5] Train loss=0.778, Valid RMSE=0.943\n",
      "[Epoch 5/5] Train loss=0.754, Valid RMSE=0.944\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T07:40:07.123516Z",
     "start_time": "2025-05-22T07:40:06.944866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred_fn = lambda sids, pids: model(torch.from_numpy(sids).to(device), torch.from_numpy(pids).to(device)).clamp(1, 5).cpu().numpy()\n",
    "\n",
    "# Evaluate on validation data\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_df, pred_fn)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ],
   "id": "52f28a4643fbf730",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.955\n"
     ]
    }
   ],
   "execution_count": 101
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
